{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuned Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=1\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_VISIBLE_DEVICES=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = \"cpu\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PubMedBert"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = load_dataset(\"paul-ww/ei-abstract-significance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_labels = ds[\"train\"].features[\"label\"]\n",
        "label2id = {name: class_labels.str2int(name) for name in class_labels.names}\n",
        "id2label = {v: k for k, v in label2id.items()}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tracking using Weights&Biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: WANDB_LOG_MODEL='end'\n",
            "env: WANDB_WATCH='all'\n"
          ]
        }
      ],
      "source": [
        "%env WANDB_LOG_MODEL='end'\n",
        "%env WANDB_WATCH='all'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"model\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"num_epochs\": 50,\n",
        "    \"weighted_loss\": True,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"seed\": 42,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaul_ww\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/Paul.Wullenweber/wandb/run-20231123_164917-j800jft6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/paul_ww/significance_classification/runs/j800jft6' target=\"_blank\">robust-waterfall-78</a></strong> to <a href='https://wandb.ai/paul_ww/significance_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/paul_ww/significance_classification' target=\"_blank\">https://wandb.ai/paul_ww/significance_classification</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/paul_ww/significance_classification/runs/j800jft6' target=\"_blank\">https://wandb.ai/paul_ww/significance_classification/runs/j800jft6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "run = wandb.init(\n",
        "    project=\"significance_classification\", group=\"transformer_finetuned\", config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    wandb.config[\"model\"], model_max_length=512, truncation_side=\"left\"\n",
        ")\n",
        "\n",
        "\n",
        "def tokenize_function(ds):\n",
        "    return tokenizer(ds[\"text\"], padding=\"max_length\", truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_tokenized = ds.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    wandb.config[\"model\"],\n",
        "    num_labels=class_labels.num_classes,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ").to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)[\n",
        "        \"accuracy\"\n",
        "    ]\n",
        "    precision_score = precision.compute(predictions=predictions, references=labels)[\n",
        "        \"precision\"\n",
        "    ]\n",
        "    recall_score = recall.compute(predictions=predictions, references=labels)[\"recall\"]\n",
        "    f1_score = f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score,\n",
        "        \"precision\": precision_score,\n",
        "        \"recall\": recall_score,\n",
        "        \"f1\": f1_score,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    report_to=\"wandb\",\n",
        "    output_dir=\"models/pubmedbert_effect\",\n",
        "    learning_rate=wandb.config[\"learning_rate\"],\n",
        "    weight_decay=wandb.config[\"weight_decay\"],\n",
        "    per_device_train_batch_size=wandb.config[\"batch_size\"],\n",
        "    per_device_eval_batch_size=wandb.config[\"batch_size\"],\n",
        "    gradient_accumulation_steps=wandb.config[\"gradient_accumulation_steps\"],\n",
        "    gradient_checkpointing=wandb.config[\"gradient_checkpointing\"],\n",
        "    num_train_epochs=wandb.config[\"num_epochs\"],\n",
        "    optim=\"adamw_torch\",\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"f1\",\n",
        "    load_best_model_at_end=True,\n",
        "    overwrite_output_dir=True,\n",
        "    seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from torch import nn\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "class_freqs = np.bincount(ds[\"train\"][\"label\"], minlength=2)\n",
        "class_freqs_inv = 1 / class_freqs\n",
        "CLASS_WEIGHTS_NORM = class_freqs_inv / class_freqs_inv.sum()\n",
        "\n",
        "\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    \"\"\"A trainer with a weighted cross-entropy-loss function.\"\"\"\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        # forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # compute custom loss (weighted by inverse class frequency)\n",
        "        loss_fct = nn.CrossEntropyLoss(\n",
        "            weight=torch.tensor(\n",
        "                CLASS_WEIGHTS_NORM, device=model.device, dtype=torch.float16\n",
        "            )\n",
        "        )\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using weighted cross entropy loss using normalized inverse class frequency. Weights: significant: 0.6264591439688716, not significant: 0.3735408560311284\n"
          ]
        }
      ],
      "source": [
        "if wandb.config[\"weighted_loss\"]:\n",
        "    print(\n",
        "        f\"Using weighted cross entropy loss using normalized inverse class frequency. Weights: significant: {CLASS_WEIGHTS_NORM[0]}, not significant: {CLASS_WEIGHTS_NORM[1]}\"\n",
        "    )\n",
        "    trainer = WeightedLossTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=ds_tokenized[\"train\"],\n",
        "        eval_dataset=ds_tokenized[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    wandb.config[\"class_weights\"] = CLASS_WEIGHTS_NORM\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds_tokenized[\"train\"],\n",
        "    eval_dataset=ds_tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Nov 23 16:49:26 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A40          On   | 00000000:01:00.0 Off |                    0 |\n",
            "|  0%   38C    P8    21W / 300W |      0MiB / 45634MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA A40          On   | 00000000:25:00.0 Off |                    0 |\n",
            "|  0%   42C    P0    76W / 300W |   1651MiB / 45634MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   2  NVIDIA A40          On   | 00000000:41:00.0 Off |                    0 |\n",
            "|  0%   35C    P8    21W / 300W |      0MiB / 45634MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   3  NVIDIA A40          On   | 00000000:81:00.0 Off |                    0 |\n",
            "|  0%   35C    P8    21W / 300W |      0MiB / 45634MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   4  NVIDIA A40          On   | 00000000:C1:00.0 Off |                    0 |\n",
            "|  0%   35C    P8    23W / 300W |      0MiB / 45634MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   5  NVIDIA A40          On   | 00000000:E1:00.0 Off |                    0 |\n",
            "|  0%   34C    P8    26W / 300W |      0MiB / 45634MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    1   N/A  N/A   2344622      C   ...er/thesis_env/bin/python3     1649MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 14:56, Epoch 47/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.643378</td>\n",
              "      <td>0.635593</td>\n",
              "      <td>0.632479</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.774869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.638072</td>\n",
              "      <td>0.635593</td>\n",
              "      <td>0.632479</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.774869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.644999</td>\n",
              "      <td>0.618644</td>\n",
              "      <td>0.643564</td>\n",
              "      <td>0.878378</td>\n",
              "      <td>0.742857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.655623</td>\n",
              "      <td>0.627119</td>\n",
              "      <td>0.702703</td>\n",
              "      <td>0.702703</td>\n",
              "      <td>0.702703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.668600</td>\n",
              "      <td>0.637352</td>\n",
              "      <td>0.677966</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.776471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.668600</td>\n",
              "      <td>0.620341</td>\n",
              "      <td>0.627119</td>\n",
              "      <td>0.633929</td>\n",
              "      <td>0.959459</td>\n",
              "      <td>0.763441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.668600</td>\n",
              "      <td>0.613242</td>\n",
              "      <td>0.644068</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.972973</td>\n",
              "      <td>0.774194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.668600</td>\n",
              "      <td>0.607817</td>\n",
              "      <td>0.703390</td>\n",
              "      <td>0.719101</td>\n",
              "      <td>0.864865</td>\n",
              "      <td>0.785276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.611900</td>\n",
              "      <td>0.600071</td>\n",
              "      <td>0.694915</td>\n",
              "      <td>0.731707</td>\n",
              "      <td>0.810811</td>\n",
              "      <td>0.769231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.611900</td>\n",
              "      <td>0.580637</td>\n",
              "      <td>0.728814</td>\n",
              "      <td>0.723404</td>\n",
              "      <td>0.918919</td>\n",
              "      <td>0.809524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.611900</td>\n",
              "      <td>0.565920</td>\n",
              "      <td>0.745763</td>\n",
              "      <td>0.729167</td>\n",
              "      <td>0.945946</td>\n",
              "      <td>0.823529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.611900</td>\n",
              "      <td>0.554556</td>\n",
              "      <td>0.745763</td>\n",
              "      <td>0.744444</td>\n",
              "      <td>0.905405</td>\n",
              "      <td>0.817073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.611900</td>\n",
              "      <td>0.544601</td>\n",
              "      <td>0.771186</td>\n",
              "      <td>0.776471</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.830189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.561000</td>\n",
              "      <td>0.528894</td>\n",
              "      <td>0.788136</td>\n",
              "      <td>0.795181</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.840764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.561000</td>\n",
              "      <td>0.481839</td>\n",
              "      <td>0.796610</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.918919</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.561000</td>\n",
              "      <td>0.460763</td>\n",
              "      <td>0.805085</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.918919</td>\n",
              "      <td>0.855346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.485100</td>\n",
              "      <td>0.438362</td>\n",
              "      <td>0.822034</td>\n",
              "      <td>0.811765</td>\n",
              "      <td>0.932432</td>\n",
              "      <td>0.867925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.485100</td>\n",
              "      <td>0.423803</td>\n",
              "      <td>0.822034</td>\n",
              "      <td>0.819277</td>\n",
              "      <td>0.918919</td>\n",
              "      <td>0.866242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.485100</td>\n",
              "      <td>0.415527</td>\n",
              "      <td>0.813559</td>\n",
              "      <td>0.825000</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.485100</td>\n",
              "      <td>0.409028</td>\n",
              "      <td>0.830508</td>\n",
              "      <td>0.855263</td>\n",
              "      <td>0.878378</td>\n",
              "      <td>0.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.485100</td>\n",
              "      <td>0.405781</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.878378</td>\n",
              "      <td>0.872483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.412000</td>\n",
              "      <td>0.397892</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.878378</td>\n",
              "      <td>0.872483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.412000</td>\n",
              "      <td>0.389851</td>\n",
              "      <td>0.855932</td>\n",
              "      <td>0.860759</td>\n",
              "      <td>0.918919</td>\n",
              "      <td>0.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.412000</td>\n",
              "      <td>0.388194</td>\n",
              "      <td>0.847458</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.918919</td>\n",
              "      <td>0.883117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.412000</td>\n",
              "      <td>0.391779</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.874172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.365900</td>\n",
              "      <td>0.395030</td>\n",
              "      <td>0.830508</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.868421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.365900</td>\n",
              "      <td>0.397107</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.878378</td>\n",
              "      <td>0.872483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.365900</td>\n",
              "      <td>0.393199</td>\n",
              "      <td>0.830508</td>\n",
              "      <td>0.855263</td>\n",
              "      <td>0.878378</td>\n",
              "      <td>0.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.323900</td>\n",
              "      <td>0.384682</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.848101</td>\n",
              "      <td>0.905405</td>\n",
              "      <td>0.875817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.323900</td>\n",
              "      <td>0.382645</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.848101</td>\n",
              "      <td>0.905405</td>\n",
              "      <td>0.875817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.323900</td>\n",
              "      <td>0.381839</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.848101</td>\n",
              "      <td>0.905405</td>\n",
              "      <td>0.875817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.323900</td>\n",
              "      <td>0.380409</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.848101</td>\n",
              "      <td>0.905405</td>\n",
              "      <td>0.875817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.323900</td>\n",
              "      <td>0.379968</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.848101</td>\n",
              "      <td>0.905405</td>\n",
              "      <td>0.875817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.275300</td>\n",
              "      <td>0.380095</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.848101</td>\n",
              "      <td>0.905405</td>\n",
              "      <td>0.875817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.275300</td>\n",
              "      <td>0.380517</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.848101</td>\n",
              "      <td>0.905405</td>\n",
              "      <td>0.875817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.275300</td>\n",
              "      <td>0.383549</td>\n",
              "      <td>0.838983</td>\n",
              "      <td>0.848101</td>\n",
              "      <td>0.905405</td>\n",
              "      <td>0.875817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.275300</td>\n",
              "      <td>0.384128</td>\n",
              "      <td>0.830508</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.868421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.270100</td>\n",
              "      <td>0.385645</td>\n",
              "      <td>0.830508</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.868421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.270100</td>\n",
              "      <td>0.386333</td>\n",
              "      <td>0.830508</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.868421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.270100</td>\n",
              "      <td>0.386550</td>\n",
              "      <td>0.830508</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.868421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.270100</td>\n",
              "      <td>0.387204</td>\n",
              "      <td>0.830508</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.868421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.270100</td>\n",
              "      <td>0.387277</td>\n",
              "      <td>0.830508</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.868421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.247300</td>\n",
              "      <td>0.387203</td>\n",
              "      <td>0.830508</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.868421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=0.4221226191520691, metrics={'train_runtime': 905.6673, 'train_samples_per_second': 56.754, 'train_steps_per_second': 0.11, 'total_flos': 1.27293128583168e+16, 'train_loss': 0.4221226191520691, 'epoch': 47.06})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "trainer.save_model(Path(run.dir) / \"model_finetuned\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predictions_proba = trainer.predict(ds_tokenized[\"test\"]).predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Module containing useful functions in the significance classification context.\"\"\"\n",
        "\n",
        "from typing import Any, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import wandb\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def log_metrics_to_wandb(\n",
        "    y_true_num: Sequence[int],\n",
        "    y_pred_proba: np.ndarray,\n",
        "    id2label: dict[int, str],\n",
        "    labels: list[str],\n",
        "    run: Any,\n",
        ") -> None:\n",
        "    \"\"\"Log binary classification metrics to Weights&Biases.\"\"\"\n",
        "    y_pred_num = np.argmax(y_pred_proba, axis=1)\n",
        "    y_true_str = [id2label[e] for e in y_true_num]\n",
        "    y_pred_str = [id2label[e] for e in y_pred_num]\n",
        "    # Confusion Matrix\n",
        "    cm = wandb.plot.confusion_matrix(\n",
        "        y_true=y_true_num, preds=y_pred_num, class_names=labels\n",
        "    )\n",
        "    wandb.log({\"test_cm\": cm})\n",
        "    # PR-Curve\n",
        "    wandb.log({\"test_pr\": wandb.plot.pr_curve(y_true_num, y_pred_proba, labels)})\n",
        "    # ROC Curve\n",
        "    wandb.log({\"test_roc\": wandb.plot.roc_curve(y_true_num, y_pred_proba, labels)})\n",
        "    # Log predicted probabilities\n",
        "    wandb.log(\n",
        "        {\n",
        "            \"test_probas\": wandb.Table(\n",
        "                data=y_pred_proba, columns=[\"prob_significant\", \"prob_not_significant\"]\n",
        "            )\n",
        "        }\n",
        "    )\n",
        "    # Additional Metrics\n",
        "    report = classification_report(\n",
        "        y_pred=y_pred_str, y_true=y_true_str, output_dict=True\n",
        "    )\n",
        "    wandb.log({\"test\": report})\n",
        "    # Ensure summary metrics are present\n",
        "    run.summary.update({\"test\": report})\n",
        "    run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d80800bc5408487a8afeed62fdf3d19d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.029 MB of 0.033 MB uploaded\\r'), FloatProgress(value=0.8845244275249466, max=1.0\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>\u2582\u2582\u2581\u2581\u2583\u2582\u2583\u2584\u2583\u2584\u2585\u2585\u2586\u2586\u2586\u2587\u2587\u2587\u2587\u2588\u2588\u2588\u2588\u2588\u2587\u2587\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2587\u2587\u2587\u2587\u2587</td></tr><tr><td>eval/f1</td><td>\u2584\u2584\u2583\u2581\u2584\u2584\u2584\u2584\u2584\u2585\u2585\u2586\u2586\u2586\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2588\u2588\u2587\u2587\u2587\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2587\u2587\u2587\u2587\u2587</td></tr><tr><td>eval/loss</td><td>\u2588\u2588\u2588\u2588\u2588\u2587\u2587\u2587\u2587\u2586\u2585\u2585\u2585\u2584\u2584\u2582\u2582\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581</td></tr><tr><td>eval/precision</td><td>\u2581\u2581\u2581\u2583\u2583\u2581\u2583\u2584\u2584\u2584\u2584\u2585\u2586\u2586\u2586\u2586\u2587\u2587\u2588\u2588\u2588\u2588\u2588\u2588\u2587\u2588\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587</td></tr><tr><td>eval/recall</td><td>\u2588\u2588\u2585\u2581\u2585\u2587\u2587\u2585\u2584\u2586\u2586\u2585\u2585\u2585\u2586\u2586\u2586\u2585\u2585\u2585\u2585\u2586\u2586\u2585\u2585\u2585\u2586\u2586\u2586\u2586\u2586\u2586\u2586\u2586\u2586\u2585\u2585\u2585\u2585\u2585</td></tr><tr><td>eval/runtime</td><td>\u2584\u2582\u2585\u2583\u2582\u2582\u2582\u2582\u2582\u2581\u2581\u2582\u2582\u2582\u2588\u2582\u2582\u2581\u2583\u2583\u2583\u2582\u2582\u2582\u2582\u2582\u2583\u2583\u2582\u2583\u2581\u2582\u2582\u2581\u2582\u2582\u2582\u2584\u2582\u2582</td></tr><tr><td>eval/samples_per_second</td><td>\u2585\u2587\u2584\u2586\u2587\u2587\u2587\u2587\u2587\u2588\u2588\u2587\u2587\u2587\u2581\u2587\u2587\u2588\u2586\u2586\u2586\u2587\u2587\u2587\u2587\u2587\u2586\u2586\u2587\u2586\u2588\u2587\u2587\u2588\u2587\u2587\u2587\u2585\u2587\u2587</td></tr><tr><td>eval/steps_per_second</td><td>\u2585\u2587\u2584\u2586\u2587\u2587\u2587\u2587\u2587\u2588\u2588\u2587\u2587\u2587\u2581\u2587\u2587\u2588\u2586\u2586\u2586\u2587\u2587\u2587\u2587\u2587\u2586\u2586\u2587\u2586\u2588\u2587\u2587\u2588\u2587\u2587\u2587\u2585\u2587\u2587</td></tr><tr><td>train/epoch</td><td>\u2581\u2581\u2581\u2582\u2582\u2582\u2582\u2582\u2582\u2583\u2583\u2583\u2583\u2583\u2584\u2584\u2584\u2584\u2584\u2584\u2585\u2585\u2585\u2585\u2585\u2586\u2586\u2586\u2586\u2586\u2587\u2587\u2587\u2587\u2587\u2587\u2588\u2588\u2588\u2588</td></tr><tr><td>train/global_step</td><td>\u2581\u2581\u2581\u2582\u2582\u2582\u2582\u2582\u2583\u2583\u2583\u2583\u2583\u2584\u2584\u2584\u2584\u2584\u2585\u2585\u2585\u2585\u2585\u2586\u2586\u2586\u2586\u2586\u2587\u2587\u2587\u2587\u2587\u2588\u2588\u2588\u2588\u2588\u2588\u2588</td></tr><tr><td>train/learning_rate</td><td>\u2588\u2587\u2586\u2586\u2585\u2584\u2583\u2583\u2582\u2581</td></tr><tr><td>train/loss</td><td>\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581\u2581\u2581</td></tr><tr><td>train/total_flos</td><td>\u2581</td></tr><tr><td>train/train_loss</td><td>\u2581</td></tr><tr><td>train/train_runtime</td><td>\u2581</td></tr><tr><td>train/train_samples_per_second</td><td>\u2581</td></tr><tr><td>train/train_steps_per_second</td><td>\u2581</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.83051</td></tr><tr><td>eval/f1</td><td>0.86842</td></tr><tr><td>eval/loss</td><td>0.3872</td></tr><tr><td>eval/precision</td><td>0.84615</td></tr><tr><td>eval/recall</td><td>0.89189</td></tr><tr><td>eval/runtime</td><td>0.5451</td></tr><tr><td>eval/samples_per_second</td><td>216.477</td></tr><tr><td>eval/steps_per_second</td><td>3.669</td></tr><tr><td>train/epoch</td><td>47.06</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2473</td></tr><tr><td>train/total_flos</td><td>1.27293128583168e+16</td></tr><tr><td>train/train_loss</td><td>0.42212</td></tr><tr><td>train/train_runtime</td><td>905.6673</td></tr><tr><td>train/train_samples_per_second</td><td>56.754</td></tr><tr><td>train/train_steps_per_second</td><td>0.11</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">robust-waterfall-78</strong> at: <a href='https://wandb.ai/paul_ww/significance_classification/runs/j800jft6' target=\"_blank\">https://wandb.ai/paul_ww/significance_classification/runs/j800jft6</a><br/>Synced 5 W&B file(s), 4 media file(s), 4 artifact file(s) and 8 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20231123_164917-j800jft6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from classification.utils import log_metrics_to_wandb\n",
        "\n",
        "log_metrics_to_wandb(\n",
        "    y_pred_proba=predictions_proba,\n",
        "    y_true_num=ds[\"test\"][\"label\"],\n",
        "    id2label=id2label,\n",
        "    labels=class_labels.names,\n",
        "    run=run,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
