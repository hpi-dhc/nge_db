{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Significance Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using the selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_DIR = Path(\"wandb_models\")\n",
    "#MODEL_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api = wandb.Api()\n",
    "\n",
    "#runs = api.runs(\n",
    "#    path=\"paul_ww/significance_classification\",\n",
    "#    filters={\"group\": \"transformer_finetuned\"},\n",
    "#    order=\"-summary_metrics.test.macro_avg_f1-score\",\n",
    "#)\n",
    "#best_run = runs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_files = [f for f in best_run.files() if f.name.startswith(\"model_finetuned\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for f in model_files:\n",
    "#    f.download(root=MODEL_DIR, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = Path('../../output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_INFERENCE_DATASET = Path(base_path / \"ids_to_abstracts_for_inference.parquet\")\n",
    "PATH_INFERENCE_RESULTS_CACHE = Path(base_path / \"prediction_results.jsonl\")\n",
    "PATH_INFERENCE_RESULTS = Path(base_path / \"ids_to_significance_predictions_finetuned.parquet\")\n",
    "MODEL = '../../models/signficance/model_finetuned'\n",
    "DEVICE = \"cuda:3\"\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_parquet(PATH_INFERENCE_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm_id</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12411355</td>\n",
       "      <td>OBJECTIVE\\n\\n\\nTo measure the effect of giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20386478</td>\n",
       "      <td>The goal of this research project was to inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20386477</td>\n",
       "      <td>The objectives of the present investigation we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20386476</td>\n",
       "      <td>The present study investigated the influence o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20386475</td>\n",
       "      <td>The purpose of the present study was to examin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776980</th>\n",
       "      <td>21594665</td>\n",
       "      <td>Trastuzumab (T) is effective in metastatic bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776981</th>\n",
       "      <td>19513541</td>\n",
       "      <td>PI-103, the first synthetic multitargeted comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776982</th>\n",
       "      <td>23094721</td>\n",
       "      <td>Regular use of aspirin after a diagnosis of co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776983</th>\n",
       "      <td>18772396</td>\n",
       "      <td>Glioblastoma multiforme (GBM) is the most comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776984</th>\n",
       "      <td>28817404</td>\n",
       "      <td>We report 2 primary renal sarcomas demonstrati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>776985 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pm_id                                           abstract\n",
       "0       12411355  OBJECTIVE\\n\\n\\nTo measure the effect of giving...\n",
       "1       20386478  The goal of this research project was to inves...\n",
       "2       20386477  The objectives of the present investigation we...\n",
       "3       20386476  The present study investigated the influence o...\n",
       "4       20386475  The purpose of the present study was to examin...\n",
       "...          ...                                                ...\n",
       "776980  21594665  Trastuzumab (T) is effective in metastatic bre...\n",
       "776981  19513541  PI-103, the first synthetic multitargeted comp...\n",
       "776982  23094721  Regular use of aspirin after a diagnosis of co...\n",
       "776983  18772396  Glioblastoma multiforme (GBM) is the most comm...\n",
       "776984  28817404  We report 2 primary renal sarcomas demonstrati...\n",
       "\n",
       "[776985 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\n",
    "    MODEL,\n",
    "    truncation=True,\n",
    "    truncation_side=\"left\",\n",
    "    model_max_length=512,\n",
    "    padding=\"max_length\",\n",
    ")\n",
    "\n",
    "MODEL = AutoModelForSequenceClassification.from_pretrained(MODEL).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "PIPE = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=MODEL,\n",
    "    tokenizer=TOKENIZER,\n",
    "    device=DEVICE,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from transformers.pipelines import Pipeline\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def predict_significance_labels(\n",
    "    ds: Dataset,\n",
    "    id_col: str,\n",
    "    feature_col: str,\n",
    "    pipe: Pipeline,\n",
    "    batch_size: int,\n",
    "    output_file: str,\n",
    "    output_batch_size: int,\n",
    ") -> None:\n",
    "    output_file = Path(output_file)\n",
    "    if output_file.exists():\n",
    "        with jsonlines.open(output_file) as reader:\n",
    "            processed_ids = [item.get(id_col) for item in reader]\n",
    "\n",
    "    else:\n",
    "        processed_ids = []\n",
    "\n",
    "    filtered_ds = ds.filter(lambda row: row[id_col] not in processed_ids)\n",
    "\n",
    "    batch = []\n",
    "    for doc_id, pred in tqdm(\n",
    "        zip(\n",
    "            filtered_ds[id_col],\n",
    "            pipe(\n",
    "                KeyDataset(filtered_ds, feature_col),\n",
    "                batch_size=batch_size,\n",
    "                truncation=True,\n",
    "                return_all_scores=True,\n",
    "            ),\n",
    "        ),\n",
    "        desc=\"Running inference\",\n",
    "        total=len(filtered_ds),\n",
    "    ):\n",
    "        batch.append(\n",
    "            {\n",
    "                id_col: doc_id,\n",
    "                \"labels\": [pred[0][\"label\"], pred[1][\"label\"]],\n",
    "                \"scores\": [pred[0][\"score\"], pred[1][\"score\"]],\n",
    "            }\n",
    "        )\n",
    "        processed_ids.append(doc_id)\n",
    "\n",
    "        if len(batch) == output_batch_size:\n",
    "            with jsonlines.open(output_file, mode=\"a\") as writer:\n",
    "                writer.write_all(batch)\n",
    "            batch = []\n",
    "\n",
    "    with jsonlines.open(output_file, mode=\"a\") as writer:\n",
    "        writer.write_all(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406c26aa54964394a2ba09035ab16c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/776985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Florian.Borchert/miniconda3/envs/nge/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Running inference: 100%|███████████████████████████████████████████████████████████████████████████████| 3442/3442 [00:32<00:00, 105.11it/s]\n"
     ]
    }
   ],
   "source": [
    "predict_significance_labels(\n",
    "    ds=ds,\n",
    "    id_col=\"pm_id\",\n",
    "    feature_col=\"abstract\",\n",
    "    pipe=PIPE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    output_file=PATH_INFERENCE_RESULTS_CACHE,\n",
    "    output_batch_size=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = list(MODEL.config.id2label.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no significant effect', 'significant effect']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm_id</th>\n",
       "      <th>scores</th>\n",
       "      <th>prob_no significant effect</th>\n",
       "      <th>prob_significant effect</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>has_significant_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2180658</td>\n",
       "      <td>[0.7253651022911071, 0.27463486790657005]</td>\n",
       "      <td>0.725365</td>\n",
       "      <td>0.274635</td>\n",
       "      <td>no significant effect</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18043056</td>\n",
       "      <td>[0.349804937839508, 0.650195062160491]</td>\n",
       "      <td>0.349805</td>\n",
       "      <td>0.650195</td>\n",
       "      <td>significant effect</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18042987</td>\n",
       "      <td>[0.545411705970764, 0.45458829402923506]</td>\n",
       "      <td>0.545412</td>\n",
       "      <td>0.454588</td>\n",
       "      <td>no significant effect</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18042976</td>\n",
       "      <td>[0.871544778347015, 0.128455191850662]</td>\n",
       "      <td>0.871545</td>\n",
       "      <td>0.128455</td>\n",
       "      <td>no significant effect</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18042975</td>\n",
       "      <td>[0.27974003553390503, 0.7202599644660951]</td>\n",
       "      <td>0.279740</td>\n",
       "      <td>0.720260</td>\n",
       "      <td>significant effect</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776980</th>\n",
       "      <td>38493933</td>\n",
       "      <td>[0.8496014475822441, 0.15039856731891602]</td>\n",
       "      <td>0.849601</td>\n",
       "      <td>0.150399</td>\n",
       "      <td>no significant effect</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776981</th>\n",
       "      <td>38490284</td>\n",
       "      <td>[0.20785824954509702, 0.792141735553741]</td>\n",
       "      <td>0.207858</td>\n",
       "      <td>0.792142</td>\n",
       "      <td>significant effect</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776982</th>\n",
       "      <td>38467383</td>\n",
       "      <td>[0.070790067315101, 0.9292099475860591]</td>\n",
       "      <td>0.070790</td>\n",
       "      <td>0.929210</td>\n",
       "      <td>significant effect</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776983</th>\n",
       "      <td>38466872</td>\n",
       "      <td>[0.19682666659355103, 0.8031733036041261]</td>\n",
       "      <td>0.196827</td>\n",
       "      <td>0.803173</td>\n",
       "      <td>significant effect</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776984</th>\n",
       "      <td>38490277</td>\n",
       "      <td>[0.7145774364471431, 0.285422563552856]</td>\n",
       "      <td>0.714577</td>\n",
       "      <td>0.285423</td>\n",
       "      <td>no significant effect</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>776985 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pm_id                                     scores  \\\n",
       "0        2180658  [0.7253651022911071, 0.27463486790657005]   \n",
       "1       18043056     [0.349804937839508, 0.650195062160491]   \n",
       "2       18042987   [0.545411705970764, 0.45458829402923506]   \n",
       "3       18042976     [0.871544778347015, 0.128455191850662]   \n",
       "4       18042975  [0.27974003553390503, 0.7202599644660951]   \n",
       "...          ...                                        ...   \n",
       "776980  38493933  [0.8496014475822441, 0.15039856731891602]   \n",
       "776981  38490284   [0.20785824954509702, 0.792141735553741]   \n",
       "776982  38467383    [0.070790067315101, 0.9292099475860591]   \n",
       "776983  38466872  [0.19682666659355103, 0.8031733036041261]   \n",
       "776984  38490277    [0.7145774364471431, 0.285422563552856]   \n",
       "\n",
       "        prob_no significant effect  prob_significant effect  \\\n",
       "0                         0.725365                 0.274635   \n",
       "1                         0.349805                 0.650195   \n",
       "2                         0.545412                 0.454588   \n",
       "3                         0.871545                 0.128455   \n",
       "4                         0.279740                 0.720260   \n",
       "...                            ...                      ...   \n",
       "776980                    0.849601                 0.150399   \n",
       "776981                    0.207858                 0.792142   \n",
       "776982                    0.070790                 0.929210   \n",
       "776983                    0.196827                 0.803173   \n",
       "776984                    0.714577                 0.285423   \n",
       "\n",
       "              predicted_label  has_significant_effect  \n",
       "0       no significant effect                   False  \n",
       "1          significant effect                    True  \n",
       "2       no significant effect                   False  \n",
       "3       no significant effect                   False  \n",
       "4          significant effect                    True  \n",
       "...                       ...                     ...  \n",
       "776980  no significant effect                   False  \n",
       "776981     significant effect                    True  \n",
       "776982     significant effect                    True  \n",
       "776983     significant effect                    True  \n",
       "776984  no significant effect                   False  \n",
       "\n",
       "[776985 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.read_json(PATH_INFERENCE_RESULTS_CACHE, lines=True).drop(\n",
    "    columns=[\"labels\"]\n",
    ")\n",
    "df_results[f\"prob_{LABELS[0]}\"] = df_results[\"scores\"].str[0]\n",
    "df_results[f\"prob_{LABELS[1]}\"] = df_results[\"scores\"].str[1]\n",
    "df_results[\"predicted_label\"] = df_results[\"scores\"].apply(\n",
    "    lambda x: LABELS[x.index(max(x))]\n",
    ")\n",
    "df_results[\"has_significant_effect\"] = df_results[f\"prob_{LABELS[1]}\"] >= 0.5\n",
    "df_results.drop(columns=\"scores\").to_parquet(PATH_INFERENCE_RESULTS, compression=\"gzip\")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nge]",
   "language": "python",
   "name": "conda-env-nge-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
